# üåê Alopecosa Fabrilis Web Interface

A beautiful, modern web interface for controlling and monitoring your spider-inspired web crawler. This interface provides an intuitive way to manage crawlers, view results, and monitor crawling progress in real-time.

## ‚ú® Features

### üé® Beautiful Design
- **Modern UI**: Glass-morphism design with spider-themed aesthetics
- **Responsive Layout**: Works perfectly on desktop, tablet, and mobile
- **Dark Theme**: Easy on the eyes with beautiful gradients and effects
- **Spider Web Background**: Subtle web pattern for authentic spider feel

### üï∑Ô∏è Crawler Management
- **Create Crawlers**: Easy form to set up new crawling jobs
- **Real-time Monitoring**: Live updates on crawling progress
- **Multiple Crawlers**: Run several crawlers simultaneously
- **Status Tracking**: Visual indicators for ready, running, completed, and error states

### üìä Dashboard & Analytics
- **Live Statistics**: Real-time counts of active crawlers, pages, and links
- **Progress Bars**: Visual progress tracking for running crawlers
- **Crawl History**: Complete history of all crawling sessions
- **Interactive Charts**: Beautiful charts showing crawling statistics

### üîÑ Real-time Updates
- **WebSocket Integration**: Instant updates without page refresh
- **Live Progress**: Real-time progress bars and status updates
- **Toast Notifications**: Beautiful notifications for all events
- **Auto-refresh**: Automatic updates every 5 seconds

## üöÄ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Launch the Web Interface

```bash
# Option 1: Use the launcher script
python3 launch_web.py

# Option 2: Run directly
python3 web_interface.py
```

### 3. Open Your Browser

Navigate to: **http://localhost:5000**

## üéØ How to Use

### Creating a New Crawler

1. **Fill out the form** in the "Create New Crawler" section:
   - **Crawler Name**: Give your spider a memorable name
   - **Target URL**: The website you want to crawl
   - **Max Depth**: How deep to explore (1-10)
   - **Max Pages**: Maximum pages to crawl (1-1000)

2. **Click "Create Crawler"** - your spider will be ready to hunt!

### Starting a Crawl

1. **Find your crawler** in the "Active Crawlers" section
2. **Click "Start Crawling"** to begin the hunt
3. **Watch the progress** with real-time updates
4. **View results** when crawling completes

### Managing Results

- **View Results**: Click "View Results" to see detailed crawl data
- **Save Results**: Download results as JSON files
- **Delete Crawlers**: Remove completed crawlers to keep things tidy

## üèóÔ∏è Architecture

### Backend (Python/Flask)
- **Flask Web Server**: Handles HTTP requests and API endpoints
- **Flask-SocketIO**: Manages real-time WebSocket connections
- **Alopecosa Crawler**: Integrates with your existing spider crawler
- **Threading**: Runs crawlers in background threads

### Frontend (HTML/JavaScript)
- **Tailwind CSS**: Modern, responsive styling framework
- **Socket.IO Client**: Real-time communication with server
- **Chart.js**: Beautiful data visualization
- **Font Awesome**: Professional icons

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/crawlers` | GET | List all crawlers |
| `/api/crawlers` | POST | Create new crawler |
| `/api/crawlers/{id}/start` | POST | Start a crawler |
| `/api/crawlers/{id}/status` | GET | Get crawler status |
| `/api/crawlers/{id}/results` | GET | Get crawl results |
| `/api/crawlers/{id}/save` | POST | Save results to file |
| `/api/crawlers/{id}` | DELETE | Delete a crawler |
| `/api/history` | GET | Get crawl history |

## üé® Customization

### Styling
The interface uses Tailwind CSS with custom CSS classes. You can modify:

- **Colors**: Update the color scheme in the CSS variables
- **Animations**: Adjust the pulse-glow and hover effects
- **Layout**: Modify the grid system and spacing
- **Background**: Change the spider web pattern

### Functionality
Extend the interface by:

- **Adding new API endpoints** in `web_interface.py`
- **Creating new dashboard widgets** in the HTML template
- **Implementing additional crawler features** in the manager class
- **Adding new visualization charts** using Chart.js

## üîß Configuration

### Environment Variables
```bash
# Set custom port (default: 5000)
export FLASK_PORT=8080

# Set debug mode
export FLASK_DEBUG=true

# Set secret key
export FLASK_SECRET_KEY=your-secret-key
```

### Web Interface Settings
Modify `web_interface.py` to adjust:

- **Port number**: Change the default port (5000)
- **Host binding**: Modify host binding (0.0.0.0)
- **Debug mode**: Enable/disable debug features
- **Secret key**: Set custom Flask secret key

## üêõ Troubleshooting

### Common Issues

#### Port Already in Use
```bash
# Check what's using port 5000
lsof -i :5000

# Kill the process or use a different port
export FLASK_PORT=8080
```

#### Dependencies Missing
```bash
# Install all requirements
pip install -r requirements.txt

# Check specific packages
pip list | grep -E "(Flask|SocketIO|requests|beautifulsoup4)"
```

#### Web Interface Won't Start
```bash
# Check Python version (should be 3.7+)
python3 --version

# Verify file permissions
chmod +x launch_web.py

# Run with verbose output
python3 -v web_interface.py
```

### Debug Mode
Enable debug mode to see detailed error messages:

```python
# In web_interface.py
socketio.run(app, debug=True, host='0.0.0.0', port=5000)
```

## üì± Mobile Support

The web interface is fully responsive and works great on mobile devices:

- **Touch-friendly**: Large buttons and touch targets
- **Responsive layout**: Adapts to different screen sizes
- **Mobile navigation**: Easy-to-use interface on small screens
- **Progressive Web App**: Can be installed on mobile devices

## üîí Security Considerations

### Production Deployment
For production use, consider:

- **HTTPS**: Use SSL/TLS encryption
- **Authentication**: Add user login system
- **Rate Limiting**: Prevent abuse of the API
- **Input Validation**: Sanitize all user inputs
- **CORS**: Configure cross-origin requests properly

### Development Mode
The current setup is for development:

- **Debug enabled**: Shows detailed error messages
- **No authentication**: Anyone can access the interface
- **HTTP only**: No encryption (use HTTPS in production)

## üöÄ Performance Tips

### Optimization
- **Limit concurrent crawlers**: Don't run too many at once
- **Monitor memory usage**: Large crawls can use significant memory
- **Use appropriate delays**: Respect website rate limits
- **Clean up old data**: Remove completed crawlers regularly

### Scaling
For high-traffic scenarios:

- **Load balancing**: Use multiple web servers
- **Database storage**: Store results in a database instead of memory
- **Background workers**: Use Celery or similar for crawler management
- **Caching**: Implement Redis for session storage

## ü§ù Contributing

### Adding Features
1. **Fork the repository**
2. **Create a feature branch**
3. **Implement your changes**
4. **Test thoroughly**
5. **Submit a pull request**

### Code Style
- **Python**: Follow PEP 8 guidelines
- **JavaScript**: Use ES6+ features
- **HTML**: Semantic markup with accessibility
- **CSS**: Consistent naming conventions

## üìö Related Documentation

- **[README.md](README.md)**: Main project overview
- **[usage.md](usage.md)**: Command-line usage guide
- **[INDEX.md](INDEX.md)**: Documentation navigation
- **[example_usage.py](example_usage.py)**: Python API examples

## üéâ Getting Help

### Support Channels
- **GitHub Issues**: Report bugs and request features
- **Documentation**: Check the comprehensive guides
- **Examples**: Review the working examples
- **Community**: Join discussions and share ideas

### Debugging Tips
1. **Check browser console** for JavaScript errors
2. **Review server logs** for Python errors
3. **Verify network requests** in browser dev tools
4. **Test API endpoints** directly with curl or Postman

---

**üï∑Ô∏è Happy crawling with your beautiful web interface!**

The Alopecosa Fabrilis now has a stunning web presence to match its intelligent crawling capabilities! üï∏Ô∏è‚ú®
